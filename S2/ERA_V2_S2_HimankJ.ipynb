{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himank-J/ERAV2/blob/main/S2/ERA_V2_S2_HimankJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "3d0a2b56-3533-41a1-eef4-529602581940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# torch: Core PyTorch library\n",
        "# torch.nn: Building blocks for neural networks\n",
        "# torch.nn.functional: Activation functions and utility functions\n",
        "# torch.optim: Optimization algorithms for training\n",
        "# torchvision.datasets: Commonly used datasets\n",
        "# torchvision.transforms: Image transformations\n",
        "# torchsummary: Visualizing neural network architectures\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checks if a CUDA-enabled GPU is available on the system.\n",
        "# If it is, the code sets the device to use the GPU for computations.\n",
        "# Otherwise, it sets the device to use the CPU.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "1831957d-c22e-4403-8b7f-0aad32f4fff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Training data loader:\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    # Load MNIST dataset for training\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    # Apply transformations to each data point\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),  # Convert data to PyTorch tensor\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data\n",
        "                    ])),\n",
        "    batch_size=batch_size,  # Set batch size\n",
        "    shuffle=True  # Shuffle the data for randomness during training\n",
        ")\n",
        "\n",
        "# Testing data loader:\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    # Load MNIST dataset for testing\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                    # Apply the same transformations as the training data loader\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size,  # Set batch size\n",
        "    shuffle=True  # Shuffle the data for randomness during testing (usually set to False for testing)\n",
        ")"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstDNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FirstDNN, self).__init__()\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "    # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "    # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out:16 , n_out:7 , j_out:4\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:16 , n_in:7 , j_in:4 , s:1 , r_out:24 , n_out:5 , j_out:4\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "    # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "    # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out:1 , j_out:4\n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "    # Added fully connected layer\n",
        "    self.fc1 = nn.Linear(10, 10)\n",
        "\n",
        "\n",
        "# Correct values\n",
        "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "    x = self.conv7(x)\n",
        "    x = F.relu(x) # this is the last step. Think what ReLU does to our results at this stage!\n",
        "    # After above step, all negatives are converted to 0 while remaining tensors are unaffected.\n",
        "    x = x.view(-1, 10) # reshapes the array x to have a single column with 10 rows\n",
        "    x = self.fc1(x)\n",
        "    # x = self.dropout(x)\n",
        "    return F.log_softmax(x)\n",
        "    # The softmax function is a common activation function used in neural networks.\n",
        "    # It takes a vector of real values as input and normalizes it into a probability distribution consisting of values between 0 and 1 that sum to 1.\n",
        "    # The log_softmax function calculates the logarithm of the softmax values. This is useful in certain scenarios such as when training neural networks, as it can improve the numerical stability of the model and make it easier to optimize.\n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstDNN().to(device)"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "8122e115-34c2-4519-e9e9-17284968aec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "           Linear-10                   [-1, 10]             110\n",
            "================================================================\n",
            "Total params: 6,379,896\n",
            "Trainable params: 6,379,896\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-498d202e6d3d>:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the model to training mode\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)  # Move the input data and target labels to the specified device (GPU in this case)\n",
        "        optimizer.zero_grad() # Zero out the gradients in the optimizer\n",
        "        output = model(data) # Forward pass: compute predicted outputs by passing input data through the model\n",
        "        loss = F.nll_loss(output, target) # Calculate the negative log likelihood (NLL) loss between the predicted output and target labels\n",
        "        loss.backward() # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        optimizer.step() # Update the weights using the optimizer\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "\n",
        "    # Initialize variables to keep track of test loss and correct predictions\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # Disable gradient computation during testing\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data) # Forward pass: compute predicted outputs by passing input data through the model\n",
        "\n",
        "            # Calculate the negative log likelihood (NLL) loss between the predicted output and target labels\n",
        "            # Sum up batch loss (reduction='sum') for later calculation of average loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # Count the number of correct predictions\n",
        "\n",
        "    test_loss /= len(test_loader.dataset) # Calculate average test loss\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SGD aims to minimize the cost or loss function by iteratively adjusting the model parameters.**\n",
        "\n",
        "***Batch Gradient Descent vs. Stochastic Gradient Descent:***\n",
        "\n",
        "- In standard gradient descent, the entire training dataset is used to compute the gradient of the cost function with respect to the model parameters. This is known as batch gradient descent.\n",
        "- In contrast, SGD uses only a subset or a single randomly chosen data point (mini-batch) to compute the gradient at each iteration. This introduces a stochastic (random) element, and it is known as stochastic gradient descent.\n",
        "\n",
        "***Advantages:***\n",
        "\n",
        "* SGD is computationally efficient as it processes only a subset of the training data at each iteration.\n",
        "* It can navigate through rough, non-convex optimization landscapes.\n",
        "\n",
        "***Disadvantages:***\n",
        "\n",
        "* It introduces noise due to the random selection of mini-batches, which can cause oscillations in the optimization process.\n",
        "* The learning rate needs to be carefully tuned.\n",
        "\n"
      ],
      "metadata": {
        "id": "vk8vne_4ITzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "1c331832-c13f-4fb6-bfea-a46b59dfd595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-87-498d202e6d3d>:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.13163171708583832 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0982, Accuracy: 9673/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "\n",
        "* Initial accuracy - 60%\n",
        "* When added dropout - max accuracy - 88% (when value was 0.2 and 0.5)\n",
        "* When added a fully connected layer with 10 neurons - accuracy 98% (without dropout layer)\n",
        "* Adding dropout layer along with Fully connected layer showed no improvement in accuracy."
      ],
      "metadata": {
        "id": "7ROu5n0YNWBf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btF-NZoYMNJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}